{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5649dffc-b579-45de-b73a-c92827ef713a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2508/2508 [18:22<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Ranked 4967 resumes. Top 20 saved to:\n",
      " - vectorizer_pkl: C:\\Users\\sagni\\Downloads\\Resume Ranker\\vectorizer.pkl\n",
      " - embeddings_h5: C:\\Users\\sagni\\Downloads\\Resume Ranker\\resume_embeddings.h5\n",
      " - rankings_json: C:\\Users\\sagni\\Downloads\\Resume Ranker\\rankings.json\n",
      " - rankings_csv: C:\\Users\\sagni\\Downloads\\Resume Ranker\\rankings.csv\n",
      " - config_yaml: C:\\Users\\sagni\\Downloads\\Resume Ranker\\config.yaml\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>name</th>\n",
       "      <th>source</th>\n",
       "      <th>top_skills</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.103024</td>\n",
       "      <td>20824105</td>\n",
       "      <td>pdf</td>\n",
       "      <td>aws, bash, c, linux, python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.103024</td>\n",
       "      <td>csv_resume_298</td>\n",
       "      <td>csv</td>\n",
       "      <td>aws, bash, c, linux, python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.095727</td>\n",
       "      <td>21297521</td>\n",
       "      <td>pdf</td>\n",
       "      <td>aws, azure, c, c#, c++, computer vision, deep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.095727</td>\n",
       "      <td>csv_resume_2199</td>\n",
       "      <td>csv</td>\n",
       "      <td>aws, azure, c, c#, c++, computer vision, deep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.079948</td>\n",
       "      <td>csv_resume_1737</td>\n",
       "      <td>csv</td>\n",
       "      <td>c, node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.079709</td>\n",
       "      <td>55595908</td>\n",
       "      <td>pdf</td>\n",
       "      <td>c, node</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.074273</td>\n",
       "      <td>csv_resume_929</td>\n",
       "      <td>csv</td>\n",
       "      <td>aws, c, go, java, jira, pandas, python, redshi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.074234</td>\n",
       "      <td>csv_resume_926</td>\n",
       "      <td>csv</td>\n",
       "      <td>bert, c, c#, css, flask, go, html, java, javas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.074234</td>\n",
       "      <td>62994611</td>\n",
       "      <td>pdf</td>\n",
       "      <td>bert, c, c#, css, flask, go, html, java, javas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.073967</td>\n",
       "      <td>11813872</td>\n",
       "      <td>pdf</td>\n",
       "      <td>aws, c, go, java, jira, pandas, python, redshi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank     score             name source  \\\n",
       "0     1  0.103024         20824105    pdf   \n",
       "1     2  0.103024   csv_resume_298    csv   \n",
       "2     3  0.095727         21297521    pdf   \n",
       "3     4  0.095727  csv_resume_2199    csv   \n",
       "4     5  0.079948  csv_resume_1737    csv   \n",
       "5     6  0.079709         55595908    pdf   \n",
       "6     7  0.074273   csv_resume_929    csv   \n",
       "7     8  0.074234   csv_resume_926    csv   \n",
       "8     9  0.074234         62994611    pdf   \n",
       "9    10  0.073967         11813872    pdf   \n",
       "\n",
       "                                          top_skills  \n",
       "0                        aws, bash, c, linux, python  \n",
       "1                        aws, bash, c, linux, python  \n",
       "2  aws, azure, c, c#, c++, computer vision, deep ...  \n",
       "3  aws, azure, c, c#, c++, computer vision, deep ...  \n",
       "4                                            c, node  \n",
       "5                                            c, node  \n",
       "6  aws, c, go, java, jira, pandas, python, redshi...  \n",
       "7  bert, c, c#, css, flask, go, html, java, javas...  \n",
       "8  bert, c, c#, css, flask, go, html, java, javas...  \n",
       "9  aws, c, go, java, jira, pandas, python, redshi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# If needed, install once:\n",
    "# !pip install pandas numpy scikit-learn pdfminer.six docx2txt PyYAML h5py tqdm\n",
    "\n",
    "import os, re, json, yaml, h5py, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "\n",
    "from pdfminer.high_level import extract_text as pdf_extract_text\n",
    "try:\n",
    "    import docx2txt\n",
    "    HAS_DOCX = True\n",
    "except Exception:\n",
    "    HAS_DOCX = False\n",
    "    warnings.warn(\"docx2txt not installed; DOCX parsing will be skipped.\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG — edit as needed\n",
    "# =========================\n",
    "BASE_DIR = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\"\n",
    "PDF_DIR  = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\\archive (1)\\data\\data\"\n",
    "CSV_PATH = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\\archive (1)\\Resume\\Resume.csv\"\n",
    "\n",
    "JD_TEXT = (\"Looking for an ML/Data Scientist with strong Python, NLP, TensorFlow or PyTorch, \"\n",
    "           \"Docker/Kubernetes, and cloud (AWS/GCP/Azure). Experience with MLOps a plus.\")\n",
    "TOP_K = 20  # how many top results to save\n",
    "\n",
    "ARTIFACT_DIR = BASE_DIR\n",
    "Path(ARTIFACT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Lightweight Skills DB\n",
    "# =========================\n",
    "SKILLS = {\n",
    "    \"programming\": [\"python\",\"c++\",\"java\",\"c\",\"c#\",\"javascript\",\"typescript\",\"go\",\"rust\",\"scala\",\"matlab\",\"sql\"],\n",
    "    \"data_ml\": [\"machine learning\",\"deep learning\",\"nlp\",\"computer vision\",\"tensorflow\",\"pytorch\",\"keras\",\n",
    "                \"scikit-learn\",\"pandas\",\"numpy\",\"opencv\",\"transformers\",\"bert\",\"xgboost\",\"lightgbm\",\"catboost\"],\n",
    "    \"cloud_devops\": [\"aws\",\"gcp\",\"azure\",\"docker\",\"kubernetes\",\"ci/cd\",\"jenkins\",\"terraform\",\"mlops\",\"sagemaker\",\"vertex ai\"],\n",
    "    \"tools\": [\"git\",\"linux\",\"bash\",\"jira\",\"tableau\",\"power bi\",\"airflow\",\"dbt\",\"spark\",\"hadoop\",\"snowflake\",\"bigquery\",\"redshift\"],\n",
    "    \"web\": [\"react\",\"node\",\"flask\",\"django\",\"streamlit\",\"fastapi\",\"html\",\"css\"]\n",
    "}\n",
    "SKILL_FLAT = sorted({s.lower() for v in SKILLS.values() for s in v})\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def read_text_file(path: Path) -> str:\n",
    "    try:\n",
    "        return Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            return Path(path).read_text(encoding=\"latin-1\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text or \"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        return pdf_extract_text(str(pdf_path))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_docx(docx_path: Path) -> str:\n",
    "    if not HAS_DOCX:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return docx2txt.process(str(docx_path)) or \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_skills(text: str) -> list:\n",
    "    txt = (text or \"\").lower()\n",
    "    found = [s for s in SKILL_FLAT if s in txt]\n",
    "    return sorted(list(set(found)))\n",
    "\n",
    "def safe_stem_filename(name: str, max_len: int = 80) -> str:\n",
    "    s = re.sub(r\"[^A-Za-z0-9._\\- ]+\", \"\", name).strip()\n",
    "    return s[:max_len] if len(s) > max_len else s\n",
    "\n",
    "def read_csv_robust(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Try multiple encodings and pandas kw combos for maximum compatibility.\"\"\"\n",
    "    encodings = [\"utf-8\", \"utf-16\", \"latin-1\", \"cp1252\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        # Try with encoding_errors first (newer pandas)\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, encoding_errors=\"ignore\", engine=\"python\")\n",
    "        except TypeError:\n",
    "            # Older pandas: no encoding_errors kw\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc, engine=\"python\")\n",
    "            except Exception as e2:\n",
    "                last_err = e2\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if last_err:\n",
    "        raise last_err\n",
    "    # Fallback empty\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def load_csv_resumes(csv_path: str) -> pd.DataFrame:\n",
    "    if not Path(csv_path).exists():\n",
    "        return pd.DataFrame(columns=[\"id\",\"name\",\"source\",\"text_raw\"])\n",
    "    df = read_csv_robust(csv_path)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"id\",\"name\",\"source\",\"text_raw\"])\n",
    "\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    possible_text_cols_priority = [\n",
    "        [\"Resume\"],\n",
    "        [\"Resume_str\"],\n",
    "        [\"resume_text\"],\n",
    "        [\"Resume\",\"skills\",\"education\",\"experience\"],\n",
    "        [\"resume\",\"skills\",\"education\",\"experience\"]\n",
    "    ]\n",
    "\n",
    "    text = None\n",
    "    for cols in possible_text_cols_priority:\n",
    "        if all(c in df.columns for c in cols):\n",
    "            text = df[cols].astype(str).agg(\" \".join, axis=1)\n",
    "            break\n",
    "\n",
    "    if text is None:\n",
    "        str_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "        if not str_cols:\n",
    "            return pd.DataFrame(columns=[\"id\",\"name\",\"source\",\"text_raw\"])\n",
    "        text = df[str_cols].astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "    name = None\n",
    "    for nc in [\"Name\",\"Candidate Name\",\"name\",\"full_name\",\"title\"]:\n",
    "        if nc in df.columns:\n",
    "            name = df[nc].astype(str)\n",
    "            break\n",
    "    if name is None:\n",
    "        name = pd.Series([f\"csv_resume_{i}\" for i in range(len(df))])\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"id\": [f\"csv_{i}\" for i in range(len(df))],\n",
    "        \"name\": name,\n",
    "        \"source\": \"csv\",\n",
    "        \"text_raw\": text\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def load_pdf_dir_resumes(pdf_dir: str) -> pd.DataFrame:\n",
    "    p = Path(pdf_dir)\n",
    "    if not p.exists():\n",
    "        return pd.DataFrame(columns=[\"id\",\"name\",\"source\",\"text_raw\"])\n",
    "\n",
    "    records = []\n",
    "    files = list(p.rglob(\"*\"))\n",
    "    for file in tqdm(files, desc=\"Parsing files\"):\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "        ext = file.suffix.lower()\n",
    "        text = \"\"\n",
    "        if ext == \".pdf\":\n",
    "            text = extract_text_from_pdf(file)\n",
    "        elif ext in (\".docx\",\".doc\"):\n",
    "            text = extract_text_from_docx(file)\n",
    "        else:\n",
    "            continue\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        records.append({\n",
    "            \"id\": f\"file_{len(records)}\",\n",
    "            \"name\": safe_stem_filename(file.stem),\n",
    "            \"source\": file.suffix.lower().lstrip(\".\"),\n",
    "            \"text_raw\": text\n",
    "        })\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "def build_corpus(res_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if res_df.empty:\n",
    "        return res_df\n",
    "    df = res_df.copy()\n",
    "    df[\"text\"] = df[\"text_raw\"].astype(str).map(clean_text)\n",
    "    df[\"skills\"] = df[\"text\"].map(extract_skills)\n",
    "    return df\n",
    "\n",
    "def vectorize_and_rank(df: pd.DataFrame, jd_text: str):\n",
    "    docs = df[\"text\"].tolist() + [clean_text(jd_text)]\n",
    "    vectorizer = TfidfVectorizer(max_features=50000, ngram_range=(1,2), stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(docs)\n",
    "    X_res, X_jd = X[:-1], X[-1]\n",
    "    sim = cosine_similarity(X_res, X_jd)[:, 0]\n",
    "    return vectorizer, X_res, X_jd, sim\n",
    "\n",
    "def save_artifacts(vectorizer, X_res, df_ranked, jd_text: str):\n",
    "    timestamp = datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # 1) Vectorizer (.pkl)\n",
    "    pkl_path = os.path.join(ARTIFACT_DIR, \"vectorizer.pkl\")\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "\n",
    "    # 2) Embeddings (.h5)\n",
    "    h5_path = os.path.join(ARTIFACT_DIR, \"resume_embeddings.h5\")\n",
    "    with h5py.File(h5_path, \"w\") as h5:\n",
    "        arr = X_res.toarray() if hasattr(X_res, \"toarray\") else np.asarray(X_res)\n",
    "        h5.create_dataset(\"tfidf_vectors\", data=arr)\n",
    "        h5.attrs[\"resume_names_json\"] = json.dumps(df_ranked[\"name\"].astype(str).tolist())\n",
    "        h5.attrs[\"resume_ids_json\"]   = json.dumps(df_ranked[\"id\"].astype(str).tolist())\n",
    "        h5.attrs[\"created_utc\"]       = timestamp\n",
    "\n",
    "    # 3) Rankings (.json and .csv)\n",
    "    json_path = os.path.join(ARTIFACT_DIR, \"rankings.json\")\n",
    "    ranked_for_json = df_ranked[[\"rank\",\"score\",\"name\",\"id\",\"source\",\"top_skills\"]].to_dict(orient=\"records\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(ranked_for_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    csv_path = os.path.join(ARTIFACT_DIR, \"rankings.csv\")\n",
    "    df_ranked.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # 4) Config (.yaml)\n",
    "    cfg = {\n",
    "        \"created_utc\": timestamp,\n",
    "        \"base_dir\": BASE_DIR,\n",
    "        \"pdf_dir\": PDF_DIR,\n",
    "        \"csv_path\": CSV_PATH,\n",
    "        \"artifacts\": {\n",
    "            \"vectorizer_pkl\": pkl_path,\n",
    "            \"embeddings_h5\": h5_path,\n",
    "            \"rankings_json\": json_path,\n",
    "            \"rankings_csv\": csv_path\n",
    "        },\n",
    "        \"tfidf\": {\"max_features\": 50000, \"ngram_range\": [1,2], \"stop_words\": \"english\"},\n",
    "        \"jd_preview\": jd_text[:500]\n",
    "    }\n",
    "    with open(os.path.join(ARTIFACT_DIR, \"config.yaml\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "    return {\n",
    "        \"vectorizer_pkl\": pkl_path,\n",
    "        \"embeddings_h5\": h5_path,\n",
    "        \"rankings_json\": json_path,\n",
    "        \"rankings_csv\": csv_path,\n",
    "        \"config_yaml\": os.path.join(ARTIFACT_DIR, \"config.yaml\")\n",
    "    }\n",
    "\n",
    "# =========================\n",
    "# PIPELINE (runs now)\n",
    "# =========================\n",
    "# 1) Load inputs\n",
    "csv_df  = load_csv_resumes(CSV_PATH)\n",
    "file_df = load_pdf_dir_resumes(PDF_DIR)\n",
    "\n",
    "resumes_df = pd.concat([csv_df, file_df], ignore_index=True)\n",
    "if resumes_df.empty:\n",
    "    raise SystemExit(\"No resumes found. Check CSV/PDF paths and try again.\")\n",
    "\n",
    "# 2) Build corpus\n",
    "corpus_df = build_corpus(resumes_df)\n",
    "\n",
    "# 3) Vectorize + rank\n",
    "vectorizer, X_res, X_jd, sim = vectorize_and_rank(corpus_df, JD_TEXT)\n",
    "corpus_df[\"score\"] = sim\n",
    "corpus_df[\"top_skills\"] = corpus_df[\"skills\"].apply(lambda s: \", \".join(s[:15]) if s else \"\")\n",
    "\n",
    "corpus_df = corpus_df.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "corpus_df.insert(0, \"rank\", corpus_df.index + 1)\n",
    "\n",
    "top_df = corpus_df.head(max(1, int(TOP_K))).copy()\n",
    "\n",
    "# 4) Save artifacts\n",
    "paths = save_artifacts(vectorizer, X_res, top_df, JD_TEXT)\n",
    "\n",
    "# 5) Display summary\n",
    "print(f\"[OK] Ranked {len(corpus_df)} resumes. Top {len(top_df)} saved to:\")\n",
    "for k, v in paths.items():\n",
    "    print(f\" - {k}: {v}\")\n",
    "\n",
    "# Show top 10 in notebook (preview)\n",
    "display_cols = [\"rank\",\"score\",\"name\",\"source\",\"top_skills\"]\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(top_df[display_cols].head(10))\n",
    "except Exception:\n",
    "    print(top_df[display_cols].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8376a-3de7-4426-bf88-cb44cb01e0d4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
