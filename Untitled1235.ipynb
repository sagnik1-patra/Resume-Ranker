{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980bd1f3-6ac0-4fe5-821c-33022aab90f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: pdfminer.six in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (20250506)\n",
      "Collecting python-docx\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting docx2txt\n",
      "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.14.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pdfminer.six) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pdfminer.six) (45.0.6)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-docx) (6.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sagni\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Downloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
      "Installing collected packages: docx2txt, python-docx\n",
      "\n",
      "   -------------------- ------------------- 1/2 [python-docx]\n",
      "   -------------------- ------------------- 1/2 [python-docx]\n",
      "   -------------------- ------------------- 1/2 [python-docx]\n",
      "   ---------------------------------------- 2/2 [python-docx]\n",
      "\n",
      "Successfully installed docx2txt-0.9 python-docx-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy scikit-learn pdfminer.six python-docx docx2txt PyYAML h5py tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5857f24-f04f-477c-9ea1-ef3a59f7d182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--jd JD] [--jd_file JD_FILE] [--top_k TOP_K]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\sagni\\AppData\\Roaming\\jupyter\\runtime\\kernel-8c6446d5-4033-434c-b916-26bbbef478b2.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "r\"\"\"\n",
    "AI Resume Ranker (TF-IDF based)\n",
    "- Reads:\n",
    "    1) PDFs/DOCX from: C:\\Users\\sagni\\Downloads\\Resume Ranker\\archive (1)\\data\\data\n",
    "    2) CSV from    : C:\\Users\\sagni\\Downloads\\Resume Ranker\\archive (1)\\Resume\\Resume.csv\n",
    "- Ranks resumes vs a Job Description (JD) text or file\n",
    "- Saves artifacts to: C:\\Users\\sagni\\Downloads\\Resume Ranker\n",
    "    - vectorizer.pkl (Pickle)\n",
    "    - resume_embeddings.h5 (HDF5)\n",
    "    - config.yaml (YAML)\n",
    "    - rankings.json (JSON)\n",
    "    - rankings.csv (CSV)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "import h5py\n",
    "import argparse\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Text & ML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "\n",
    "# File parsing\n",
    "from pdfminer.high_level import extract_text as pdf_extract_text\n",
    "try:\n",
    "    import docx2txt\n",
    "    HAS_DOCX = True\n",
    "except ImportError:\n",
    "    HAS_DOCX = False\n",
    "    warnings.warn(\"docx2txt not installed; DOCX parsing will be skipped.\")\n",
    "\n",
    "# -----------------------\n",
    "# Paths (Windows raw strings)\n",
    "# -----------------------\n",
    "BASE_DIR = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\"\n",
    "PDF_DIR  = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\\archive (1)\\data\\data\"\n",
    "CSV_PATH = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\\archive (1)\\Resume\\Resume.csv\"\n",
    "\n",
    "ARTIFACT_DIR = BASE_DIR  # saving here as requested\n",
    "\n",
    "# -----------------------\n",
    "# Light skill dictionary (extend as you like)\n",
    "# -----------------------\n",
    "SKILLS = {\n",
    "    \"programming\": [\n",
    "        \"python\", \"c++\", \"java\", \"c\", \"c#\", \"javascript\", \"typescript\", \"go\", \"rust\", \"scala\", \"matlab\", \"sql\"\n",
    "    ],\n",
    "    \"data_ml\": [\n",
    "        \"machine learning\", \"deep learning\", \"nlp\", \"computer vision\", \"tensorflow\", \"pytorch\",\n",
    "        \"keras\", \"scikit-learn\", \"pandas\", \"numpy\", \"opencv\", \"transformers\", \"bert\", \"xgboost\"\n",
    "    ],\n",
    "    \"cloud_devops\": [\n",
    "        \"aws\", \"gcp\", \"azure\", \"docker\", \"kubernetes\", \"ci/cd\", \"jenkins\", \"terraform\"\n",
    "    ],\n",
    "    \"tools\": [\n",
    "        \"git\", \"linux\", \"bash\", \"jira\", \"tableau\", \"power bi\"\n",
    "    ],\n",
    "    \"web\": [\n",
    "        \"react\", \"node\", \"flask\", \"django\", \"streamlit\", \"fastapi\", \"html\", \"css\"\n",
    "    ]\n",
    "}\n",
    "SKILL_FLAT = sorted({s.lower() for v in SKILLS.values() for s in v})\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def read_text_file(path: Path) -> str:\n",
    "    try:\n",
    "        return Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            return Path(path).read_text(encoding=\"latin-1\", errors=\"ignore\")\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text or \"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        return pdf_extract_text(str(pdf_path))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_docx(docx_path: Path) -> str:\n",
    "    if not HAS_DOCX:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return docx2txt.process(str(docx_path)) or \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_skills(text: str) -> list:\n",
    "    text_low = text.lower()\n",
    "    found = []\n",
    "    for s in SKILL_FLAT:\n",
    "        if s in text_low:\n",
    "            found.append(s)\n",
    "    return sorted(list(set(found)))\n",
    "\n",
    "def safe_stem_filename(name: str, max_len: int = 80) -> str:\n",
    "    s = re.sub(r\"[^A-Za-z0-9._\\- ]+\", \"\", name).strip()\n",
    "    return s[:max_len] if len(s) > max_len else s\n",
    "\n",
    "def load_csv_resumes(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust loading for common Kaggle Resume.csv schemas.\n",
    "    Returns columns: ['id','name','source','text_raw']\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, encoding=\"utf-8\", errors=\"ignore\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    possible_text_cols_priority = [\n",
    "        [\"Resume\"],\n",
    "        [\"Resume_str\"],\n",
    "        [\"resume_text\"],\n",
    "        [\"Resume\", \"skills\", \"education\", \"experience\"],\n",
    "        [\"resume\", \"skills\", \"education\", \"experience\"]\n",
    "    ]\n",
    "\n",
    "    text = None\n",
    "    for cols in possible_text_cols_priority:\n",
    "        if all(c in df.columns for c in cols):\n",
    "            text = df[cols].astype(str).agg(\" \".join, axis=1)\n",
    "            break\n",
    "\n",
    "    if text is None:\n",
    "        str_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "        if not str_cols:\n",
    "            raise ValueError(\"Could not find textual columns in CSV.\")\n",
    "        text = df[str_cols].astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "    name = None\n",
    "    for nc in [\"Name\", \"Candidate Name\", \"name\", \"full_name\", \"title\"]:\n",
    "        if nc in df.columns:\n",
    "            name = df[nc].astype(str)\n",
    "            break\n",
    "    if name is None:\n",
    "        name = pd.Series([f\"csv_resume_{i}\" for i in range(len(df))])\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"id\": [f\"csv_{i}\" for i in range(len(df))],\n",
    "        \"name\": name,\n",
    "        \"source\": \"csv\",\n",
    "        \"text_raw\": text\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def load_pdf_dir_resumes(pdf_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Walk PDF_DIR and DOCX files, parse text.\n",
    "    Returns columns: ['id','name','source','text_raw']\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    p = Path(pdf_dir)\n",
    "    if not p.exists():\n",
    "        return pd.DataFrame(columns=[\"id\", \"name\", \"source\", \"text_raw\"])\n",
    "\n",
    "    for file in tqdm(list(p.rglob(\"*\")), desc=\"Parsing files\"):\n",
    "        if not file.is_file():\n",
    "            continue\n",
    "        ext = file.suffix.lower()\n",
    "        text = \"\"\n",
    "        if ext == \".pdf\":\n",
    "            text = extract_text_from_pdf(file)\n",
    "        elif ext in (\".docx\", \".doc\"):\n",
    "            text = extract_text_from_docx(file)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if not text.strip():\n",
    "            continue\n",
    "\n",
    "        records.append({\n",
    "            \"id\": f\"file_{len(records)}\",\n",
    "            \"name\": safe_stem_filename(file.stem),\n",
    "            \"source\": file.suffix.lower().lstrip(\".\"),\n",
    "            \"text_raw\": text\n",
    "        })\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "def build_corpus(res_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = res_df.copy()\n",
    "    df[\"text\"] = df[\"text_raw\"].astype(str).map(clean_text)\n",
    "    df[\"skills\"] = df[\"text\"].map(extract_skills)\n",
    "    return df\n",
    "\n",
    "def vectorize_and_rank(df: pd.DataFrame, jd_text: str):\n",
    "    # TF-IDF on resumes + JD\n",
    "    docs = df[\"text\"].tolist() + [clean_text(jd_text)]\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=50000,\n",
    "        ngram_range=(1,2),\n",
    "        stop_words=\"english\"\n",
    "    )\n",
    "    X = vectorizer.fit_transform(docs)  # shape: (N_resumes+1, V)\n",
    "    X_res = X[:-1]\n",
    "    X_jd  = X[-1]\n",
    "\n",
    "    sim = cosine_similarity(X_res, X_jd)[:, 0]  # (N_resumes, )\n",
    "    return vectorizer, X_res, X_jd, sim\n",
    "\n",
    "def save_artifacts(vectorizer, X_res, df_ranked, jd_text: str, config_path_yaml: str):\n",
    "    timestamp = datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # 1) Save vectorizer as .pkl\n",
    "    pkl_path = os.path.join(ARTIFACT_DIR, \"vectorizer.pkl\")\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(vectorizer, f)\n",
    "\n",
    "    # 2) Save embeddings to HDF5 .h5\n",
    "    h5_path = os.path.join(ARTIFACT_DIR, \"resume_embeddings.h5\")\n",
    "    with h5py.File(h5_path, \"w\") as h5:\n",
    "        if hasattr(X_res, \"toarray\"):\n",
    "            arr = X_res.toarray()\n",
    "        else:\n",
    "            arr = np.asarray(X_res)\n",
    "        h5.create_dataset(\"tfidf_vectors\", data=arr)\n",
    "\n",
    "        # metadata\n",
    "        names = df_ranked[\"name\"].astype(str).tolist()\n",
    "        ids   = df_ranked[\"id\"].astype(str).tolist()\n",
    "        h5.attrs[\"resume_names_json\"] = json.dumps(names)\n",
    "        h5.attrs[\"resume_ids_json\"]   = json.dumps(ids)\n",
    "        h5.attrs[\"created_utc\"]       = timestamp\n",
    "\n",
    "    # 3) Save rankings as JSON\n",
    "    json_path = os.path.join(ARTIFACT_DIR, \"rankings.json\")\n",
    "    ranked_for_json = df_ranked[[\"rank\", \"score\", \"name\", \"id\", \"source\", \"top_skills\"]].to_dict(orient=\"records\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(ranked_for_json, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 4) Save config as YAML\n",
    "    cfg = {\n",
    "        \"created_utc\": timestamp,\n",
    "        \"base_dir\": BASE_DIR,\n",
    "        \"pdf_dir\": PDF_DIR,\n",
    "        \"csv_path\": CSV_PATH,\n",
    "        \"artifacts\": {\n",
    "            \"vectorizer_pkl\": pkl_path,\n",
    "            \"embeddings_h5\": h5_path,\n",
    "            \"rankings_json\": json_path,\n",
    "            \"rankings_csv\": os.path.join(ARTIFACT_DIR, \"rankings.csv\")\n",
    "        },\n",
    "        \"tfidf\": {\n",
    "            \"max_features\": 50000,\n",
    "            \"ngram_range\": [1, 2],\n",
    "            \"stop_words\": \"english\"\n",
    "        },\n",
    "        \"jd_preview\": jd_text[:500]\n",
    "    }\n",
    "    with open(config_path_yaml, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump(cfg, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"AI Resume Ranker (TF-IDF)\")\n",
    "    parser.add_argument(\"--jd\", type=str, default=None, help=\"Job description text (quoted)\")\n",
    "    parser.add_argument(\"--jd_file\", type=str, default=None, help=\"Path to a JD text file\")\n",
    "    parser.add_argument(\"--top_k\", type=int, default=50, help=\"How many top results to save\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    Path(ARTIFACT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Read JD\n",
    "    if args.jd_file and Path(args.jd_file).exists():\n",
    "        jd_text = read_text_file(Path(args.jd_file))\n",
    "    elif args.jd is not None:\n",
    "        jd_text = args.jd\n",
    "    else:\n",
    "        jd_text = \"We are looking for an ML Engineer with Python, scikit-learn, NLP, and deployment experience.\"\n",
    "\n",
    "    # Load CSV resumes\n",
    "    csv_df = pd.DataFrame()\n",
    "    if Path(CSV_PATH).exists():\n",
    "        try:\n",
    "            csv_df = load_csv_resumes(CSV_PATH)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error reading CSV resumes: {e}\")\n",
    "\n",
    "    # Load PDF/DOCX resumes\n",
    "    file_df = load_pdf_dir_resumes(PDF_DIR)\n",
    "\n",
    "    # Combine\n",
    "    resumes_df = pd.concat([csv_df, file_df], ignore_index=True)\n",
    "    if resumes_df.empty:\n",
    "        raise SystemExit(\"No resumes found. Please check CSV/PDF paths.\")\n",
    "\n",
    "    # Build corpus\n",
    "    corpus_df = build_corpus(resumes_df)\n",
    "\n",
    "    # Vectorize + similarity\n",
    "    vectorizer, X_res, X_jd, sim = vectorize_and_rank(corpus_df, jd_text)\n",
    "\n",
    "    # Attach scores & skills\n",
    "    corpus_df[\"score\"] = sim\n",
    "    corpus_df[\"top_skills\"] = corpus_df[\"skills\"].apply(lambda s: \", \".join(s[:15]) if s else \"\")\n",
    "\n",
    "    # Rank\n",
    "    corpus_df = corpus_df.sort_values(\"score\", ascending=False).reset_index(drop=True)\n",
    "    corpus_df.insert(0, \"rank\", corpus_df.index + 1)\n",
    "\n",
    "    # Trim to top_k for outputs\n",
    "    top_k = max(1, int(args.top_k))\n",
    "    top_df = corpus_df.head(top_k).copy()\n",
    "\n",
    "    # Save CSV rankings\n",
    "    rankings_csv_path = os.path.join(ARTIFACT_DIR, \"rankings.csv\")\n",
    "    top_df.to_csv(rankings_csv_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # Save artifacts\n",
    "    config_yaml_path = os.path.join(ARTIFACT_DIR, \"config.yaml\")\n",
    "    save_artifacts(vectorizer, X_res, top_df, jd_text, config_yaml_path)\n",
    "\n",
    "    # Console summary\n",
    "    print(f\"\\n[OK] Ranked {len(corpus_df)} resumes. Top {top_k} saved.\")\n",
    "    print(f\"- Rankings CSV : {rankings_csv_path}\")\n",
    "    print(f\"- Rankings JSON: {os.path.join(ARTIFACT_DIR, 'rankings.json')}\")\n",
    "    print(f\"- Vectorizer   : {os.path.join(ARTIFACT_DIR, 'vectorizer.pkl')}\")\n",
    "    print(f\"- Embeddings   : {os.path.join(ARTIFACT_DIR, 'resume_embeddings.h5')}\")\n",
    "    print(f\"- Config YAML  : {config_yaml_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6eaa3-329e-486d-b1aa-601b79753187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
