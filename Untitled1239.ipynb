{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c244cb-02cb-4d3d-878e-fbec1e3afd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing scores for ALL resumes (first time only) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing files: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 2508/2508 [17:59<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved all scores → C:\\Users\\sagni\\Downloads\\Resume Ranker\\scores_full.csv\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "No overlap by id/name. Wrote unmatched labels for inspection → C:\\Users\\sagni\\Downloads\\Resume Ranker\\unmatched_labels.csv",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m No overlap by id/name. Wrote unmatched labels for inspection → C:\\Users\\sagni\\Downloads\\Resume Ranker\\unmatched_labels.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# If needed once:\n",
    "# !pip install pandas numpy scikit-learn pdfminer.six docx2txt tqdm matplotlib\n",
    "\n",
    "import os, re, warnings, difflib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "from pdfminer.high_level import extract_text as pdf_extract_text\n",
    "try:\n",
    "    import docx2txt\n",
    "    HAS_DOCX = True\n",
    "except Exception:\n",
    "    HAS_DOCX = False\n",
    "    warnings.warn(\"docx2txt not installed; DOCX parsing will be skipped.\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG — edit as needed\n",
    "# =========================\n",
    "BASE_DIR     = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\"\n",
    "PDF_DIR      = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\\archive (1)\\data\\data\"\n",
    "CSV_PATH     = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\\archive (1)\\Resume\\Resume.csv\"\n",
    "LABELS_CSV   = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\\labels.csv\"\n",
    "SCORES_FULL  = str(Path(BASE_DIR) / \"scores_full.csv\")   # cache of all resume scores\n",
    "ALIGNED_PREV = str(Path(BASE_DIR) / \"labels_aligned_preview.csv\")\n",
    "UNMATCHED    = str(Path(BASE_DIR) / \"unmatched_labels.csv\")\n",
    "\n",
    "JD_TEXT = (\"Looking for an ML/Data Scientist with strong Python, NLP, TensorFlow or PyTorch, \"\n",
    "           \"Docker/Kubernetes, and cloud (AWS/GCP/Azure). Experience with MLOps a plus.\")\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def read_csv_robust(path: str) -> pd.DataFrame:\n",
    "    encodings = [\"utf-8\", \"utf-16\", \"latin-1\", \"cp1252\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, encoding_errors=\"ignore\", engine=\"python\")\n",
    "        except TypeError:\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc, engine=\"python\")\n",
    "            except Exception as e2:\n",
    "                last_err = e2\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if last_err:\n",
    "        raise last_err\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text or \"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        return pdf_extract_text(str(pdf_path))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_docx(docx_path: Path) -> str:\n",
    "    if not HAS_DOCX:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return docx2txt.process(str(docx_path)) or \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def safe_stem_filename(name: str, max_len: int = 80) -> str:\n",
    "    s = re.sub(r\"[^A-Za-z0-9._\\- ]+\", \"\", name).strip()\n",
    "    return s[:max_len] if len(s) > max_len else s\n",
    "\n",
    "def load_csv_resumes(csv_path: str) -> pd.DataFrame:\n",
    "    if not Path(csv_path).exists():\n",
    "        return pd.DataFrame(columns=[\"id\",\"name\",\"source\",\"text_raw\"])\n",
    "    df = read_csv_robust(csv_path)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"id\",\"name\",\"source\",\"text_raw\"])\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    possible_text_cols_priority = [\n",
    "        [\"Resume\"],\n",
    "        [\"Resume_str\"],\n",
    "        [\"resume_text\"],\n",
    "        [\"Resume\",\"skills\",\"education\",\"experience\"],\n",
    "        [\"resume\",\"skills\",\"education\",\"experience\"]\n",
    "    ]\n",
    "    text = None\n",
    "    for cols in possible_text_cols_priority:\n",
    "        if all(c in df.columns for c in cols):\n",
    "            text = df[cols].astype(str).agg(\" \".join, axis=1)\n",
    "            break\n",
    "    if text is None:\n",
    "        str_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "        if not str_cols:\n",
    "            return pd.DataFrame(columns=[\"id\",\"name\",\"source\",\"text_raw\"])\n",
    "        text = df[str_cols].astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "    name = None\n",
    "    for nc in [\"Name\",\"Candidate Name\",\"name\",\"full_name\",\"title\"]:\n",
    "        if nc in df.columns:\n",
    "            name = df[nc].astype(str)\n",
    "            break\n",
    "    if name is None:\n",
    "        name = pd.Series([f\"csv_resume_{i}\" for i in range(len(df))])\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"id\": [f\"csv_{i}\" for i in range(len(df))],\n",
    "        \"name\": name,\n",
    "        \"source\": \"csv\",\n",
    "        \"text_raw\": text\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def load_pdf_dir_resumes(pdf_dir: str) -> pd.DataFrame:\n",
    "    p = Path(pdf_dir)\n",
    "    if not p.exists():\n",
    "        return pd.DataFrame(columns=[\"id\",\"name\",\"source\",\"text_raw\"])\n",
    "    records = []\n",
    "    for file in tqdm(list(p.rglob(\"*\")), desc=\"Parsing files\"):\n",
    "        if not file.is_file(): \n",
    "            continue\n",
    "        ext = file.suffix.lower()\n",
    "        if ext == \".pdf\":\n",
    "            text = extract_text_from_pdf(file)\n",
    "        elif ext in (\".docx\",\".doc\"):\n",
    "            text = extract_text_from_docx(file)\n",
    "        else:\n",
    "            continue\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        records.append({\n",
    "            \"id\": f\"file_{len(records)}\",\n",
    "            \"name\": safe_stem_filename(file.stem),\n",
    "            \"source\": ext.lstrip(\".\"),\n",
    "            \"text_raw\": text\n",
    "        })\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "def compute_scores_tfidf(texts: pd.Series, jd_text: str) -> np.ndarray:\n",
    "    docs = texts.tolist() + [clean_text(jd_text)]\n",
    "    vec = TfidfVectorizer(max_features=50000, ngram_range=(1,2), stop_words=\"english\")\n",
    "    X = vec.fit_transform(docs)\n",
    "    X_res, X_jd = X[:-1], X[-1]\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    return cosine_similarity(X_res, X_jd)[:, 0]\n",
    "\n",
    "def normalize_bool(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    s = str(val).strip().lower()\n",
    "    true_vals  = {\"1\",\"true\",\"yes\",\"y\",\"selected\",\"hire\",\"hired\",\"positive\",\"pos\",\"shortlisted\",\"good\",\"relevant\"}\n",
    "    false_vals = {\"0\",\"false\",\"no\",\"n\",\"rejected\",\"reject\",\"negative\",\"neg\",\"not selected\",\"bad\",\"irrelevant\",\"non-relevant\"}\n",
    "    if s in true_vals:  return 1\n",
    "    if s in false_vals: return 0\n",
    "    try:\n",
    "        f = float(s)\n",
    "        return 1 if f >= 0.5 else 0\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def unify_name(s: str) -> str:\n",
    "    if s is None or (isinstance(s,float) and np.isnan(s)): return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "# =========================\n",
    "# Load labels\n",
    "# =========================\n",
    "if not Path(LABELS_CSV).exists():\n",
    "    raise SystemExit(\"labels.csv not found. Create/fill it, then rerun.\")\n",
    "\n",
    "labels = read_csv_robust(LABELS_CSV)\n",
    "if labels.empty:\n",
    "    raise SystemExit(\"labels.csv is empty. Please fill labels and save.\")\n",
    "\n",
    "labels.columns = [c.strip().lower() for c in labels.columns]\n",
    "key_col = \"id\" if \"id\" in labels.columns else (\"name\" if \"name\" in labels.columns else None)\n",
    "if key_col is None:\n",
    "    raise SystemExit(\"labels.csv must have 'id' or 'name' column.\")\n",
    "\n",
    "label_col = None\n",
    "for cand in [\"label\",\"labels\",\"target\",\"y\",\"class\",\"gt\",\"ground_truth\",\"selected\",\"hired\",\"suitable\"]:\n",
    "    if cand in labels.columns:\n",
    "        label_col = cand\n",
    "        break\n",
    "if label_col is None:\n",
    "    raise SystemExit(\"labels.csv needs a label column (e.g., 'label').\")\n",
    "\n",
    "labels[\"label_bin\"] = labels[label_col].map(normalize_bool)\n",
    "labels = labels.dropna(subset=[\"label_bin\"]).copy()\n",
    "labels[\"label_bin\"] = labels[\"label_bin\"].astype(int)\n",
    "\n",
    "# =========================\n",
    "# Build/Load full scores\n",
    "# =========================\n",
    "if Path(SCORES_FULL).exists():\n",
    "    scores_df = read_csv_robust(SCORES_FULL)\n",
    "    scores_df.columns = [c.strip().lower() for c in scores_df.columns]\n",
    "    need_compute = scores_df.empty or \"score\" not in scores_df.columns\n",
    "else:\n",
    "    need_compute = True\n",
    "\n",
    "if need_compute:\n",
    "    print(\"Computing scores for ALL resumes (first time only) ...\")\n",
    "    csv_df  = load_csv_resumes(CSV_PATH)\n",
    "    file_df = load_pdf_dir_resumes(PDF_DIR)\n",
    "    resumes = pd.concat([csv_df, file_df], ignore_index=True)\n",
    "    if resumes.empty:\n",
    "        raise SystemExit(\"No resumes found to compute scores.\")\n",
    "    resumes[\"text\"] = resumes[\"text_raw\"].astype(str).map(clean_text)\n",
    "    resumes[\"score\"] = compute_scores_tfidf(resumes[\"text\"], JD_TEXT)\n",
    "    scores_df = resumes[[\"id\",\"name\",\"source\",\"score\"]].copy()\n",
    "    scores_df.to_csv(SCORES_FULL, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[OK] Saved all scores → {SCORES_FULL}\")\n",
    "\n",
    "# =========================\n",
    "# Align labels & scores (multi-strategy)\n",
    "# =========================\n",
    "# Strategy A: exact ID match\n",
    "matched_A = pd.merge(labels[[key_col,\"label_bin\"]], scores_df, how=\"inner\", left_on=key_col, right_on=\"id\") if key_col==\"id\" else pd.DataFrame()\n",
    "\n",
    "# Strategy B: normalized name match\n",
    "labels[\"name_key\"] = labels[\"name\"].astype(str).map(unify_name) if \"name\" in labels.columns else \"\"\n",
    "scores_df[\"name_key\"] = scores_df[\"name\"].astype(str).map(unify_name)\n",
    "matched_B = pd.DataFrame()\n",
    "if \"name\" in labels.columns:\n",
    "    matched_B = pd.merge(labels[[\"name_key\",\"label_bin\",\"name\"]], scores_df[[\"name_key\",\"name\",\"id\",\"score\",\"source\"]], how=\"inner\", on=\"name_key\")\n",
    "\n",
    "# Strategy C: fuzzy name suggestion (high threshold to avoid wrong joins)\n",
    "unmatched = None\n",
    "if matched_A.empty and matched_B.empty:\n",
    "    if \"name\" not in labels.columns:\n",
    "        raise SystemExit(\"No overlap by id and no 'name' column to try fuzzy matching. Add 'name' to labels.csv.\")\n",
    "    lab_unq = labels[[\"name\",\"label_bin\"]].dropna().copy()\n",
    "    lab_unq[\"name_key\"] = lab_unq[\"name\"].astype(str).map(unify_name)\n",
    "    scr_names = scores_df[[\"name\",\"id\",\"score\",\"source\"]].copy()\n",
    "    scr_names[\"name_key\"] = scr_names[\"name\"].astype(str).map(unify_name)\n",
    "\n",
    "    matches = []\n",
    "    scr_keys = scr_names[\"name_key\"].tolist()\n",
    "    for _, row in lab_unq.iterrows():\n",
    "        key = row[\"name_key\"]\n",
    "        # best 3 close matches\n",
    "        close = difflib.get_close_matches(key, scr_keys, n=3, cutoff=0.90)  # 0.90 is strict; lower to 0.80 if needed\n",
    "        if close:\n",
    "            best = close[0]\n",
    "            hit = scr_names[scr_names[\"name_key\"]==best].iloc[0]\n",
    "            matches.append({\n",
    "                \"label_name\": row[\"name\"],\n",
    "                \"label_bin\": int(row[\"label_bin\"]),\n",
    "                \"matched_name\": hit[\"name\"],\n",
    "                \"matched_id\": hit[\"id\"],\n",
    "                \"score\": hit[\"score\"],\n",
    "                \"source\": hit[\"source\"]\n",
    "            })\n",
    "    matched_C = pd.DataFrame(matches)\n",
    "    if matched_C.empty:\n",
    "        # dump unmatched names for manual inspection\n",
    "        unmatched = lab_unq[[\"name\"]].copy()\n",
    "        unmatched.to_csv(UNMATCHED, index=False, encoding=\"utf-8\")\n",
    "        raise SystemExit(f\"No overlap by id/name. Wrote unmatched labels for inspection → {UNMATCHED}\")\n",
    "else:\n",
    "    matched_C = pd.DataFrame()\n",
    "\n",
    "# Choose best available matches\n",
    "if not matched_A.empty:\n",
    "    merged = matched_A.copy()\n",
    "elif not matched_B.empty:\n",
    "    merged = matched_B.copy()\n",
    "else:\n",
    "    merged = matched_C.rename(columns={\"matched_name\":\"name\",\"matched_id\":\"id\"}) if not matched_C.empty else pd.DataFrame()\n",
    "\n",
    "if merged.empty:\n",
    "    raise SystemExit(\"Could not align labels and scores. Please inspect labels.csv vs scores_full.csv naming.\")\n",
    "\n",
    "# Save a preview of aligned rows to inspect if needed\n",
    "prev_cols = [c for c in [\"id\",\"name\",\"score\",\"source\",\"label_bin\"] if c in merged.columns]\n",
    "merged[prev_cols].to_csv(ALIGNED_PREV, index=False, encoding=\"utf-8\")\n",
    "print(f\"[OK] Aligned {len(merged)} labeled rows with scores. Preview saved → {ALIGNED_PREV}\")\n",
    "\n",
    "# =========================\n",
    "# Metrics & Plots\n",
    "# =========================\n",
    "y_true  = merged[\"label_bin\"].astype(int).values\n",
    "y_score = merged[\"score\"].astype(float).values\n",
    "\n",
    "# Accuracy & F1 vs threshold\n",
    "thresholds = np.linspace(0.0, 1.0, 201)\n",
    "accs, f1s = [], []\n",
    "for t in thresholds:\n",
    "    y_pred = (y_score >= t).astype(int)\n",
    "    accs.append(accuracy_score(y_true, y_pred))\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    f1s.append(f1)\n",
    "\n",
    "best_idx = int(np.argmax(accs))\n",
    "best_t   = float(thresholds[best_idx])\n",
    "best_acc = float(accs[best_idx])\n",
    "best_f1  = float(f1s[best_idx])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thresholds, accs, label=\"Accuracy\")\n",
    "plt.plot(thresholds, f1s, label=\"F1\")\n",
    "plt.axvline(best_t, linestyle=\"--\", label=f\"Best T={best_t:.3f}\")\n",
    "plt.title(\"Accuracy / F1 vs Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best threshold: {best_t:.3f} | Accuracy={best_acc:.4f} | F1={best_f1:.4f}\")\n",
    "\n",
    "# Confusion matrix at best threshold\n",
    "y_pred_best = (y_score >= best_t).astype(int)\n",
    "cm = confusion_matrix(y_true, y_pred_best, labels=[0,1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(cm, aspect=\"auto\")\n",
    "plt.title(\"Confusion Matrix (Heatmap)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xticks([0,1], [\"0 (Neg)\",\"1 (Pos)\"])\n",
    "plt.yticks([0,1], [\"0 (Neg)\",\"1 (Pos)\"])\n",
    "for (i,j), v in np.ndenumerate(cm):\n",
    "    plt.text(j, i, str(v), ha='center', va='center', fontsize=12)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"TN={tn}  FP={fp}  FN={fn}  TP={tp}\")\n",
    "\n",
    "# Optional: ROC & PR curves\n",
    "try:\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_score)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(rec, prec)\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Skipped ROC/PR (need both positive & negative labels):\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d564444-c5cd-4321-b444-c18bd8ad160c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
