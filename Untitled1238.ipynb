{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed2b52c-ad70-4bce-8863-8ed66d996ae2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "No overlap between labels and scores. Check IDs/names.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m No overlap between labels and scores. Check IDs/names.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagni\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# If needed once:\n",
    "# !pip install pandas numpy scikit-learn pdfminer.six docx2txt tqdm matplotlib\n",
    "\n",
    "import os, re, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "from pdfminer.high_level import extract_text as pdf_extract_text\n",
    "try:\n",
    "    import docx2txt\n",
    "    HAS_DOCX = True\n",
    "except Exception:\n",
    "    HAS_DOCX = False\n",
    "    warnings.warn(\"docx2txt not installed; DOCX parsing will be skipped.\")\n",
    "\n",
    "# =========================\n",
    "# CONFIG â€” edit as needed\n",
    "# =========================\n",
    "BASE_DIR   = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\"\n",
    "PDF_DIR    = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\\archive (1)\\data\\data\"\n",
    "CSV_PATH   = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\\archive (1)\\Resume\\Resume.csv\"\n",
    "LABELS_CSV = r\"C:\\Users\\sagni\\Downloads\\Resume Ranker\\labels.csv\"\n",
    "RANKINGS_CSV = str(Path(BASE_DIR) / \"rankings.csv\")  # optional fast-path if exists\n",
    "\n",
    "JD_TEXT = (\"Looking for an ML/Data Scientist with strong Python, NLP, TensorFlow or PyTorch, \"\n",
    "           \"Docker/Kubernetes, and cloud (AWS/GCP/Azure). Experience with MLOps a plus.\")\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def read_csv_robust(path: str) -> pd.DataFrame:\n",
    "    encodings = [\"utf-8\", \"utf-16\", \"latin-1\", \"cp1252\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, encoding_errors=\"ignore\", engine=\"python\")\n",
    "        except TypeError:\n",
    "            try:\n",
    "                return pd.read_csv(path, encoding=enc, engine=\"python\")\n",
    "            except Exception as e2:\n",
    "                last_err = e2\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    if last_err:\n",
    "        raise last_err\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = text or \"\"\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    try:\n",
    "        return pdf_extract_text(str(pdf_path))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_docx(docx_path: Path) -> str:\n",
    "    if not HAS_DOCX:\n",
    "        return \"\"\n",
    "    try:\n",
    "        return docx2txt.process(str(docx_path)) or \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def safe_stem_filename(name: str, max_len: int = 80) -> str:\n",
    "    s = re.sub(r\"[^A-Za-z0-9._\\- ]+\", \"\", name).strip()\n",
    "    return s[:max_len] if len(s) > max_len else s\n",
    "\n",
    "def load_csv_resumes(csv_path: str) -> pd.DataFrame:\n",
    "    if not Path(csv_path).exists():\n",
    "        return pd.DataFrame(columns=[\"id\",\"name\",\"source\",\"text_raw\"])\n",
    "    df = read_csv_robust(csv_path)\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(columns=[\"id\",\"name\",\"source\",\"text_raw\"])\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    possible_text_cols_priority = [\n",
    "        [\"Resume\"],\n",
    "        [\"Resume_str\"],\n",
    "        [\"resume_text\"],\n",
    "        [\"Resume\",\"skills\",\"education\",\"experience\"],\n",
    "        [\"resume\",\"skills\",\"education\",\"experience\"]\n",
    "    ]\n",
    "    text = None\n",
    "    for cols in possible_text_cols_priority:\n",
    "        if all(c in df.columns for c in cols):\n",
    "            text = df[cols].astype(str).agg(\" \".join, axis=1)\n",
    "            break\n",
    "    if text is None:\n",
    "        str_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "        if not str_cols:\n",
    "            return pd.DataFrame(columns=[\"id\",\"name\",\"source\",\"text_raw\"])\n",
    "        text = df[str_cols].astype(str).agg(\" \".join, axis=1)\n",
    "\n",
    "    name = None\n",
    "    for nc in [\"Name\",\"Candidate Name\",\"name\",\"full_name\",\"title\"]:\n",
    "        if nc in df.columns:\n",
    "            name = df[nc].astype(str)\n",
    "            break\n",
    "    if name is None:\n",
    "        name = pd.Series([f\"csv_resume_{i}\" for i in range(len(df))])\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"id\": [f\"csv_{i}\" for i in range(len(df))],\n",
    "        \"name\": name,\n",
    "        \"source\": \"csv\",\n",
    "        \"text_raw\": text\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def load_pdf_dir_resumes(pdf_dir: str) -> pd.DataFrame:\n",
    "    p = Path(pdf_dir)\n",
    "    if not p.exists():\n",
    "        return pd.DataFrame(columns=[\"id\",\"name\",\"source\",\"text_raw\"])\n",
    "    records = []\n",
    "    for file in tqdm(list(p.rglob(\"*\")), desc=\"Parsing files\"):\n",
    "        if not file.is_file(): continue\n",
    "        ext = file.suffix.lower()\n",
    "        if ext == \".pdf\":\n",
    "            text = extract_text_from_pdf(file)\n",
    "        elif ext in (\".docx\",\".doc\"):\n",
    "            text = extract_text_from_docx(file)\n",
    "        else:\n",
    "            continue\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        records.append({\n",
    "            \"id\": f\"file_{len(records)}\",\n",
    "            \"name\": safe_stem_filename(file.stem),\n",
    "            \"source\": ext.lstrip(\".\"),\n",
    "            \"text_raw\": text\n",
    "        })\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "def compute_scores_tfidf(texts: pd.Series, jd_text: str) -> np.ndarray:\n",
    "    docs = texts.tolist() + [clean_text(jd_text)]\n",
    "    vec = TfidfVectorizer(max_features=50000, ngram_range=(1,2), stop_words=\"english\")\n",
    "    X = vec.fit_transform(docs)\n",
    "    X_res, X_jd = X[:-1], X[-1]\n",
    "    # cosine similarity for normalized TF-IDF\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    return cosine_similarity(X_res, X_jd)[:, 0]\n",
    "\n",
    "def normalize_bool(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    s = str(val).strip().lower()\n",
    "    true_vals  = {\"1\",\"true\",\"yes\",\"y\",\"selected\",\"hire\",\"hired\",\"positive\",\"pos\",\"shortlisted\",\"good\",\"relevant\"}\n",
    "    false_vals = {\"0\",\"false\",\"no\",\"n\",\"rejected\",\"reject\",\"negative\",\"neg\",\"not selected\",\"bad\",\"irrelevant\",\"non-relevant\"}\n",
    "    if s in true_vals:  return 1\n",
    "    if s in false_vals: return 0\n",
    "    try:\n",
    "        f = float(s)\n",
    "        return 1 if f >= 0.5 else 0\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def unify_name(s: str) -> str:\n",
    "    if s is None or (isinstance(s,float) and np.isnan(s)): return \"\"\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"\", s)\n",
    "    return s\n",
    "\n",
    "# =========================\n",
    "# Load labels\n",
    "# =========================\n",
    "if not Path(LABELS_CSV).exists():\n",
    "    raise SystemExit(\"labels.csv not found. Create it first, then rerun.\")\n",
    "\n",
    "labels = read_csv_robust(LABELS_CSV)\n",
    "if labels.empty:\n",
    "    raise SystemExit(\"labels.csv is empty. Please fill labels and save.\")\n",
    "\n",
    "labels.columns = [c.strip().lower() for c in labels.columns]\n",
    "# choose key col\n",
    "key_col = \"id\" if \"id\" in labels.columns else (\"name\" if \"name\" in labels.columns else None)\n",
    "if key_col is None:\n",
    "    raise SystemExit(\"labels.csv must have 'id' or 'name' column.\")\n",
    "\n",
    "# pick label column\n",
    "label_col = None\n",
    "for cand in [\"label\",\"labels\",\"target\",\"y\",\"class\",\"gt\",\"ground_truth\",\"selected\",\"hired\",\"suitable\"]:\n",
    "    if cand in labels.columns:\n",
    "        label_col = cand\n",
    "        break\n",
    "if label_col is None:\n",
    "    raise SystemExit(\"labels.csv needs a label column (e.g., 'label').\")\n",
    "\n",
    "labels[\"label_bin\"] = labels[label_col].map(normalize_bool)\n",
    "labels = labels.dropna(subset=[\"label_bin\"]).copy()\n",
    "labels[\"label_bin\"] = labels[\"label_bin\"].astype(int)\n",
    "\n",
    "# =========================\n",
    "# Get/compute scores\n",
    "# =========================\n",
    "scores_df = None\n",
    "if Path(RANKINGS_CSV).exists():\n",
    "    try:\n",
    "        rnk = read_csv_robust(RANKINGS_CSV)\n",
    "        rnk.columns = [c.strip().lower() for c in rnk.columns]\n",
    "        # keep minimal\n",
    "        keep = [c for c in [\"id\",\"name\",\"score\",\"source\"] if c in rnk.columns]\n",
    "        scores_df = rnk[keep].copy()\n",
    "    except Exception:\n",
    "        scores_df = None\n",
    "\n",
    "if scores_df is None or \"score\" not in scores_df.columns or scores_df[\"score\"].isna().all():\n",
    "    # fallback: compute scores (slower; parses PDFs/DOCX)\n",
    "    print(\"Computing scores from raw resumes (this may take a while)...\")\n",
    "    csv_df  = load_csv_resumes(CSV_PATH)\n",
    "    file_df = load_pdf_dir_resumes(PDF_DIR)\n",
    "    resumes = pd.concat([csv_df, file_df], ignore_index=True)\n",
    "    if resumes.empty:\n",
    "        raise SystemExit(\"No resumes found to compute scores.\")\n",
    "    resumes[\"text\"] = resumes[\"text_raw\"].astype(str).map(clean_text)\n",
    "    resumes[\"score\"] = compute_scores_tfidf(resumes[\"text\"], JD_TEXT)\n",
    "    scores_df = resumes[[\"id\",\"name\",\"score\",\"source\"]].copy()\n",
    "\n",
    "# =========================\n",
    "# Align labels & scores\n",
    "# =========================\n",
    "if key_col == \"id\":\n",
    "    merged = pd.merge(labels[[key_col,\"label_bin\"]], scores_df, how=\"inner\", on=\"id\")\n",
    "else:\n",
    "    # name-based join with normalization for safety\n",
    "    tmp_lab = labels.copy();   tmp_lab[\"name_key\"] = tmp_lab[\"name\"].astype(str).map(unify_name)\n",
    "    tmp_sc  = scores_df.copy(); tmp_sc[\"name_key\"]  = tmp_sc[\"name\"].astype(str).map(unify_name)\n",
    "    merged = pd.merge(tmp_lab[[\"name_key\",\"label_bin\"]], tmp_sc[[\"name_key\",\"name\",\"score\",\"id\",\"source\"]], how=\"inner\", on=\"name_key\")\n",
    "\n",
    "if merged.empty:\n",
    "    raise SystemExit(\"No overlap between labels and scores. Check IDs/names.\")\n",
    "\n",
    "# =========================\n",
    "# Metrics & Plots\n",
    "# =========================\n",
    "y_true  = merged[\"label_bin\"].astype(int).values\n",
    "y_score = merged[\"score\"].astype(float).values\n",
    "\n",
    "# Accuracy & F1 vs threshold\n",
    "thresholds = np.linspace(0.0, 1.0, 201)\n",
    "accs, f1s = [], []\n",
    "for t in thresholds:\n",
    "    y_pred = (y_score >= t).astype(int)\n",
    "    accs.append(accuracy_score(y_true, y_pred))\n",
    "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    f1s.append(f1)\n",
    "\n",
    "best_idx = int(np.argmax(accs))\n",
    "best_t   = float(thresholds[best_idx])\n",
    "best_acc = float(accs[best_idx])\n",
    "best_f1  = float(f1s[best_idx])\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thresholds, accs, label=\"Accuracy\")\n",
    "plt.plot(thresholds, f1s, label=\"F1\")\n",
    "plt.axvline(best_t, linestyle=\"--\", label=f\"Best T={best_t:.3f}\")\n",
    "plt.title(\"Accuracy / F1 vs Threshold\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best threshold: {best_t:.3f} | Accuracy={best_acc:.4f} | F1={best_f1:.4f}\")\n",
    "\n",
    "# Confusion matrix at best threshold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_best = (y_score >= best_t).astype(int)\n",
    "cm = confusion_matrix(y_true, y_pred_best, labels=[0,1])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.imshow(cm, aspect=\"auto\")\n",
    "plt.title(\"Confusion Matrix (Heatmap)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xticks([0,1], [\"0 (Neg)\",\"1 (Pos)\"])\n",
    "plt.yticks([0,1], [\"0 (Neg)\",\"1 (Pos)\"])\n",
    "for (i,j), v in np.ndenumerate(cm):\n",
    "    plt.text(j, i, str(v), ha='center', va='center', fontsize=12)\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"TN={tn}  FP={fp}  FN={fn}  TP={tp}\")\n",
    "\n",
    "# Optional: ROC & PR curves\n",
    "try:\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    prec, rec, _ = precision_recall_curve(y_true, y_score)\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc:.3f}\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(rec, prec)\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Skipped ROC/PR (need both positive & negative labels):\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebcc3c9-bd50-4aa6-b486-1fe4d18a7dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (moviepy)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
